application:
  configuration:
    configuration.cookbooks_url: "https://s3.amazonaws.com/qubell-starter-kit-artifacts/qubell-bazaar/component-hadoop-cookbooks-stable-v5.tar.gz"
    configuration.repository_url: "http://archive.cloudera.com"
    configuration.cloudera_manager_version: "5.1.3"
    configuration.cloudera_hadoop_version: "5.1.3"
  interfaces:
    configuration:
      cookbooks_url: "bind(cloudera-hadoop#input.cookbooks_url)"
      repository_url: "bind(cloudera-hadoop#input.repository_url)"
      cloudera_manager_version: "bind(cloudera-hadoop#input.cloudera_manager_version)"
      cloudera_hadoop_version: "bind(cloudera-hadoop#input.cloudera_hadoop_version)"
    compute-Manager: 
      "*": "bind(cloudera-hadoop#compute-Manager.*)"
    compute-Master:  
      "*": "bind(cloudera-hadoop#compute-Master.*)"
    compute-DataNodes:
      "*": "bind(cloudera-hadoop#compute-DataNodes.*)"
    vms:
      Node_Manager_DNS: "bind(cloudera-hadoop#vms.Node_Manager_DNS)"
      Node_Master_DNS: "bind(cloudera-hadoop#vms.Node_Master_DNS)"
      DataNodesDNS: "bind(cloudera-hadoop#vms.DataNodesDNS)"
    cloudera-manager:
      Manager_URL: "bind(cloudera-hadoop#cloudera-manager.cloudera_Manager)"
      Login: "bind(cloudera-hadoop#cloudera-manager.cloudera_Login)"
      Password: "bind(cloudera-hadoop#cloudera-manager.cloudera_Password)"
    cloudera-hadoop:
      NameNode: "bind(cloudera-hadoop#result.NameNode)"
      Primary_NameNode: "bind(cloudera-hadoop#result.Primary_NameNode)"
      Secondary_NameNode: "bind(cloudera-hadoop#result.Secondary_NameNode)"
      Hbase_Master: "bind(cloudera-hadoop#result.Hbase_Master)"
      Hbase_MasterDns: "bind(cloudera-hadoop#result.Hbase_MasterDns)"
      JobTracker: "bind(cloudera-hadoop#result.JobTracker)"
      cloudera_hdfsWebui: "bind(cloudera-hadoop#result.cloudera_hdfsWebui)"
      cloudera_jobtrackerWebui: "bind(cloudera-hadoop#result.cloudera_jobtrackerWebui)"
      cloudera_hbaseWebui: "bind(cloudera-hadoop#result.cloudera_hbaseWebui)"
      application-pic: "bind(metadata#output.application-pic)"
      upload-data: "bind(cloudera-hadoop#actions.upload-data)"
      cleanup-data: "bind(cloudera-hadoop#actions.cleanup-data)"
  components:
    metadata:
      type: cobalt.common.Constants
      interfaces:
        output:
          application-pic:
            type: publish-signal(map<string, object>)
            name: ""
      configuration:
        configuration.values:
          output.application-pic:
            large: "https://s3.amazonaws.com/qubell-images/hadoop_128.png"
            small: "https://s3.amazonaws.com/qubell-images/hadoop_128.png"
            small-height: 128
    cloudera-hadoop:
      type: workflow.Instance
      interfaces:
        input:
          repository_url:
            type: configuration(string)
            name: Cloudera RPM repository
          cookbooks_url:
            type: configuration(string)
            name: Chef cookbooks
          cloudera_hadoop_version:
            type: configuration(string)
            name: Cloudera Hadoop version
          cloudera_manager_version:
            type: configuration(string)
            name: Cloudera Manager version
        actions:
          upload-data:
            type: receive-command(string archive-url => string data-dir, string data-url)
            name: Upload data
          cleanup-data:
            type: receive-command(string data-dir)
            name: Cleanup data
        compute-Master:
          networks:        consume-signal(map<string, map<string, string>>)
          exec:            send-command(string command, int timeout => string stdOut, string stdErr => string stdOut, string stdErr, int exitCode)
          put-file:        send-command(string filename, bytes payload)
          get-file:        send-command(string filename => bytes payload)
        compute-Manager:
          networks:        consume-signal(map<string, map<string, string>>)
          exec:            send-command(string command, int timeout => string stdOut, string stdErr => string stdOut, string stdErr, int exitCode)
          put-file:        send-command(string filename, bytes payload)
          get-file:        send-command(string filename => bytes payload)
        compute-DataNodes:
          networks:        consume-signal(map<string, map<string, string>>)
          exec:            send-command(string command, int timeout => string stdOut, string stdErr => string stdOut, string stdErr, int exitCode)
          put-file:        send-command(string filename, bytes payload)
          get-file:        send-command(string filename => bytes payload)
        vms:
          Node_Manager_DNS: consume-signal(string)
          Node_Master_DNS: consume-signal(string)
          DataNodesDNS: consume-signal(list<string>)
        cloudera-manager:
          cloudera_Manager: consume-signal(string)
          cloudera_Login: consume-signal(string)
          cloudera_Password: consume-signal(string)
        result:
          NameNode:
            type: publish-signal(list<string>)
            name: Default Namenode
          Primary_NameNode:
            type: publish-signal(list<string>)
            name: Primary Namenode
          Secondary_NameNode:
            type: publish-signal(list<string>)
            name: Secondary Namenode
          Hbase_Master:
            type: publish-signal(list<string>)
            name: HBase master server
          Hbase_MasterDns:
            type: publish-signal(string)
            name: HBase master server DNS
          JobTracker:
            type: publish-signal(list<string>)
            name: Jobtracker
          cloudera_hdfsWebui:
            type: publish-signal(list<string>)
            name: HDFS UI
          cloudera_jobtrackerWebui:
            type: publish-signal(list<string>)
            name: Jobtracker UI
          cloudera_hbaseWebui:
            type: publish-signal(list<string>)
            name: HBase UI
      required: [vms, cloudera-manager, compute-Manager, compute-Master, compute-DataNodes]
      configuration:
        configuration.triggers: {}
        configuration.workflows:
          launch:
            steps:
              - get-signals:
                  action: getSignals
                  parameters:
                    multi: true
                  output:
                    signals: result
              - add-cdh-repo:
                  action: chefrun
                  precedingPhases: [ get-signals ]
                  parameters:
                    isSudo: true
                    isSolo: true
                    roles: [ "compute-Master", "compute-Manager", "compute-DataNodes" ]
                    runList: ["cloudera::cdh_repo"]
                    recipeUrl: "{$.cookbooks_url}"
                    retryCount: 2
                    jattrs:
                      cloudera:
                        repository_url: "{$.repository_url}"
                        hadoop:
                          version: "{$.cloudera_hadoop_version}"
              - install-cdh-packages:
                  action: chefrun
                  precedingPhases: [ add-cdh-repo ]
                  parameters:
                    isSudo: true
                    isSolo: true
                    roles: [ "compute-Master", "compute-Manager", "compute-DataNodes" ]
                    runList: ["cloudera::cdh"]
                    recipeUrl: "{$.cookbooks_url}"
                    retryCount: 2
                    jattrs:
                      cloudera:
                        repository_url: "{$.repository_url}"
                        hadoop:
                          version: "{$.cloudera_hadoop_version}"
              - provision-hadoop:
                  action: chefrun
                  phase: provision-hadoop
                  precedingPhases: [ install-cdh-packages ]
                  parameters:
                    isSudo: true
                    isSolo: true
                    roles: [ "compute-Manager" ]
                    recipeUrl: "{$.cookbooks_url}"
                    runList: [ "recipe[cloudera::clusters_init]", "recipe[cloudera::hadoop_services_start]" ]
                    jattrs:
                      java:
                        java_home: "/usr/java/jdk6"
                      cloudera:
                        services: [ "hdfs" ]
                        manager:
                          host: "{$.signals.vms.*.Node_Manager_DNS[0]}"
                        hadoop:
                          version: "{$.cloudera_hadoop_version}"
                        clusters:
                          default:
                            services:
                              zookeeper:
                                server:
                                  hosts: [ "{$.signals.compute-Manager.*.networks.public.ip[0]}", "{$.signals.compute-Master.*.networks.public.ip[0]}", "{$.signals.compute-DataNodes.*.networks.public.ip[0]}" ]
                                  config:
                                    zk_server_log_dir: "/opt/log/zookeeper"
                              hdfs:
                                namenode:
                                  hosts: "{$.signals.compute-Master.*.networks.public.ip}"
                                  config:
                                    dfs_name_dir_list: "/srv/dfs/nn"
                                secondarynamenode:
                                  hosts: "{$.signals.compute-Manager.*.networks.public.ip}"
                                  config:
                                    fs_checkpoint_dir_list: "/srv/dfs/snn"
                                datanode:
                                  hosts: "{$.signals.compute-DataNodes.*.networks.public.ip}" 
                                  config:
                                    log_threshold: "WARN"
                                    dfs_data_dir_list: "/srv/dfs/dn"
                                    dfs_datanode_data_dir_perm: "755"
                                    datanode_config_safety_valve: |
                                                                    <property>
                                                                        <name>dfs.block.local-path-access.user</name>
                                                                        <value>hbase</value>
                                                                    </property>
                              mapreduce:
                                jobtracker:
                                  hosts: "{$.signals.compute-Manager.*.networks.public.ip}"
                                tasktracker:
                                  hosts: "{$.signals.compute-DataNodes.*.networks.public.ip}" 
                                  config:
                                    mapred_tasktracker_map_tasks_maximum: 6
                                    mapred_tasktracker_reduce_tasks_maximum: 4
                        repository_url: "{$.repository_url}"
              - init-hadoop-dirs:
                  action: execrun
                  precedingPhases: [ provision-hadoop ]
                  parameters:
                    isSudo: true
                    roles: [ "compute-Master" ]
                    command:
                        - bash
                        - '-c'
                        - |
                          su hdfs -c "hadoop fs -mkdir -p /tmp";
                          su hdfs -c "hadoop fs -chmod 1777 /tmp";
                          su hdfs -c "hadoop fs -mkdir /hbase";
                          su hdfs -c "hadoop fs -chown hbase:hbase /hbase";
                          su hdfs -c "hadoop fs -mkdir /user";
                          su hdfs -c "hadoop fs -mkdir /user/mapred";
                          su hdfs -c "hadoop fs -chown mapred:mapred /user/mapred";

              - start-hadoop:
                  action: chefrun
                  precedingPhases: [ init-hadoop-dirs ]
                  parameters:
                    isSudo: true
                    isSolo: true
                    roles: [ "compute-Manager" ]
                    recipeUrl: "{$.cookbooks_url}"
                    runList: [ "recipe[cloudera::hadoop_services_start]" ]
                    jattrs:
                      java:
                        java_home: "/usr/java/jdk6"
                      cloudera:
                        services: [ "zookeeper", "mapreduce" ]
                        manager:
                          host: "{$.signals.vms.*.Node_Manager_DNS[0]}"
                        repository_url: "{$.repository_url}"
                        hadoop:
                          version: "{$.cloudera_hadoop_version}"
              - install-hbase:
                  action: chefrun
                  precedingPhases: [ start-hadoop ]
                  parameters:
                    isSudo: true
                    isSolo: true
                    roles: [ "compute-Manager" ]
                    recipeUrl: "{$.cookbooks_url}"
                    runList: [ "recipe[cloudera::hbase]" ]
                    retryCount: 2
                    jattrs:
                      cloudera:
                        master:
                          host: "{$.signals.vms.*.Node_Master_DNS[0]}"
                        manager:
                          host: "{$.signals.vms.*.Node_Manager_DNS[0]}"
                          version: "{$.cloudera_manager_version}"
                        datanodes:
                          hosts: "{$.signals.compute-DataNodes.*.networks.public.ip}" 
                        hadoop:
                          version: "{$.cloudera_hadoop_version}"
                        repository_url: "{$.repository_url}"
            return:
              - NameNode:
                  description: "Name Node for HA if available"
                  value: "{$.signals.compute-Master.*.networks.public.ip}"
              - Primary_NameNode:
                  description: "Primary NameNode"
                  value: "{$.signals.compute-Master.*.networks.public.ip}"
              - Secondary_NameNode:
                  description: "Secondary NameNode"
                  value: "{$.signals.compute-Manager.*.networks.public.ip}"
              - Hbase_Master:
                  description: "Hbase node"
                  value: "{$.signals.compute-Manager.*.networks.public.ip}"
              - Hbase_MasterDns:
                  description: "Hbase Master Dns Name"
                  value: "{$.signals.vms.*.Node_Manager_DNS[0]}"
              - JobTracker:
                  description: "Job Tracker node"
                  value: "{$.signals.compute-Manager.*.networks.public.ip}"
              - cloudera_hdfsWebui:
                  description: "Namenode (HDFS)"
                  value: "{$.signals.compute-Master.*.networks.public.ip}:50070"
              - cloudera_jobtrackerWebui:
                  description: "Job Tracker (MapReduce)"
                  value: "{$.signals.compute-Manager.*.networks.public.ip}:50030"
              - cloudera_hbaseWebui:
                  description: "Master (HBase)"
                  value: "{$.signals.compute-Master.*.networks.public.ip}:60010"
          upload-data:
            steps:
              - get-env-props:
                  action: getEnvironmentProperties
                  output:
                    props: result
              - upload-data:
                  action: execrun
                  precedingPhases: [ get-env-props ]
                  parameters:
                    isSudo: true
                    roles: [ "compute-Master" ]
                    command:
                        - |
                           TDIR=`openssl rand -hex 10`
                           hadoop fs -rm -f -r "/tmp/$$TDIR" &>/dev/null
                           hadoop fs -mkdir -p "/tmp/$$TDIR" &>/dev/null
                           mkdir -p /tmp/$$TDIR/input &>/dev/null
                           curl -kL "{$.archive-url}" | tar -C "/tmp/$$TDIR/input" -xzf - &>/dev/null
                           hadoop fs -put "/tmp/$$TDIR/input" "/tmp/$$TDIR/input" &>/dev/null
                           rm -rf "/tmp/$$TDIR" &>/dev/null
                           echo -n "/tmp/$$TDIR"
                  output:
                    hdfs-path: stdout

            return:
              - data-dir:
                  description: HDFS path for uploaded data
                  value: "{$.hdfs-path['*'][0]}"
              - data-url:
                  description: View online
                  value: "http://{$.props.vms.*.Node_Master_DNS[0]}:50070/explorer.html#{$.hdfs-path['*'][0]}"

          cleanup-data:
            steps:
              - get-env-props:
                  action: getEnvironmentProperties
                  output:
                    props: result
              - cleanup-data:
                  action: execrun
                  precedingPhases: [ get-env-props ]
                  parameters:
                    isSudo: true
                    roles: [ "compute-Master" ]
                    command:
                        - hadoop fs -rm -f -r "{$.data-dir}" &>/dev/null
